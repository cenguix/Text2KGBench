{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7204faf9-d1b1-475f-890b-519ac0051d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c5b6466-2d46-4f27-b0ce-e14530a8cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d1d24a2-dc53-444b-8aeb-887afa3150f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_date_string(date_string):\n",
    "    pattern = r\"^(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})Z$\"\n",
    "    match = re.match(pattern, date_string)\n",
    "    if match:\n",
    "        year, month, day, hour, minute, second = match.groups()\n",
    "        date = datetime(int(year), int(month), int(day))\n",
    "        month_name = date.strftime(\"%B\")\n",
    "        new_date_string = f\"{day} {month_name} {year}\"\n",
    "        return new_date_string\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "def get_splits(triples, splits = [0.4, 0.3, 0.3]):\n",
    "    triples = np.array(triples)\n",
    "    indices = np.random.permutation(triples.shape[0])\n",
    "    train_count = int(triples.shape[0] * splits[0])\n",
    "    val_count = int(triples.shape[0] * splits[1])\n",
    "    test_count = triples.shape[0] - train_count - val_count\n",
    "    train_triples = triples[indices[:train_count]]\n",
    "    val_triples = triples[indices[train_count:train_count+val_count]]\n",
    "    test_triples = triples[indices[train_count+val_count:]]\n",
    "    return train_triples.tolist(), val_triples.tolist(), test_triples.tolist()\n",
    "\n",
    "def save_triples(onto_id, train_all, val_all, test_all):\n",
    "    with open(f\"data/{onto_id}/{onto_id}_train.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(train_all):\n",
    "            data = {\"id\": f\"{onto_id}_train_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "            \n",
    "    with open(f\"data/{onto_id}/{onto_id}_validation.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(val_all):\n",
    "            data = {\"id\": f\"{onto_id}_val_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "            \n",
    "    with open(f\"data/ground_truth/{onto_id}_ground_truth.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(test_all):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\") \n",
    "            \n",
    "    with open(f\"data/{onto_id}/{onto_id}_test.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(test_all):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": tr[6]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "    \n",
    "def get_triples_with_sentences(relation_pid: str, relation_label: str, rel_domain: str, rel_range: str, limit: int = 200):\n",
    "    assert relation_pid, \"relation id can't be empty\"\n",
    "    assert rel_domain, \"domain can't be empty\"\n",
    "    \n",
    "    ## build the SPARQL query\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\")\n",
    "    query = \"PREFIX wdt: <http://www.wikidata.org/prop/direct/> \\n PREFIX wd: <http://www.wikidata.org/entity/> \\n\"\n",
    "    query += \"SELECT DISTINCT ?sub ?subEntity ?objEntity ?objLabel { \\n ?subEntity wdt:P31/wdt:P279* wd:\" + rel_domain + \" . \\n\"\n",
    "    query += '?subEntity rdfs:label ?sub . FILTER (lang(?sub) = \"en\") \\n '\n",
    "    query += '?subEntity wdt:' + relation_pid + ' ?objEntity . \\n'\n",
    "    if rel_range and rel_range != \"\":\n",
    "        query += '?objEntity wdt:P31*/wdt:P279* wd:' + rel_range + ' . \\n '\n",
    "    query += 'OPTIONAL { ?objEntity rdfs:label ?objLabel . FILTER (lang(?objLabel) = \"en\") } \\n } '\n",
    "    # we get 15 time (we can adjuct n) more the results because we have to ignore some if we don't find sentences or repeating sub/obj values\n",
    "    query += f\"LIMIT 10000\"\n",
    "    if show_query:\n",
    "        print(query)\n",
    "        \n",
    "    if query in sparql_query_cache:\n",
    "        triples = sparql_query_cache[query] \n",
    "    else:\n",
    "        # execute the query and a get a set of triples\n",
    "        triples = list()\n",
    "        subject_counter, object_counter = Counter(), Counter()\n",
    "        secondary_triples = list()\n",
    "        sparql.setQuery(query)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        sparql.setTimeout(1500)\n",
    "        results = sparql.query().convert()\n",
    "        print(f'  {len(results[\"results\"][\"bindings\"])} SPARQL results.')\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            t_subject = result['sub']['value']\n",
    "            if 'objLabel' in result:\n",
    "                t_object = result['objLabel']['value']\n",
    "                t_object_id = result['objEntity']['value'].replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "            else:\n",
    "                t_object = result['objEntity']['value']\n",
    "                date_string = convert_date_string(t_object)\n",
    "                if date_string:\n",
    "                    t_object = date_string\n",
    "                t_object_id = None\n",
    "            t_subject_id = result['subEntity']['value'].replace(\"http://www.wikidata.org/entity/\",\"\")\n",
    "            triple = [t_subject, relation_label, t_object, t_subject_id, relation_pid, t_object_id]    \n",
    "            # in order to get a diverse dataset, we ignore subject/object if they occur more than 10% of the limit\n",
    "            subject_counter[t_subject] += 1\n",
    "            object_counter[t_object] += 1\n",
    "            if subject_counter[t_subject] > (limit / 10) or object_counter[t_object] > (limit / 10):\n",
    "                secondary_triples.append(triple)\n",
    "                continue\n",
    "            triples.append(triple)\n",
    "\n",
    "        # we just put the diverse triples in the begining and then put all the rest\n",
    "        triples += secondary_triples\n",
    "        sparql_query_cache[query] = triples\n",
    "        \n",
    "    print(f\"  collected {len(triples)} triples\")\n",
    "    if show_sample:\n",
    "        print(f\"  sample:\")\n",
    "        for tr in triples[:5]:\n",
    "            print(f\"    {tr[:3]}\")\n",
    "        \n",
    "    triples_with_sentences = list()\n",
    "    for tr in triples:\n",
    "        search_key = create_key(tr[0],tr[1], tr[2])\n",
    "        if search_key in sent_index:\n",
    "            sentence = sent_index[search_key] \n",
    "        else:\n",
    "            continue\n",
    "        tr.append(sentence)\n",
    "        triples_with_sentences.append(tr)\n",
    "        \n",
    "        # once we actually check for setences, we will stop at the limit\n",
    "        if len(triples_with_sentences) >= limit:\n",
    "            break\n",
    "            \n",
    "    return triples_with_sentences\n",
    "\n",
    "    # columns = [\"subject\", \"relation\", \"object\", \"subject_entity\", \"object_entity\", \"sentence\"]\n",
    "    # df = pd.DataFrame(triples_with_sentences, columns=columns)\n",
    "    # return df\n",
    "\n",
    "def create_key(sub_label, rel_label, obj_label):\n",
    "    # remove spaces and make lower case\n",
    "    sub_label = re.sub(r\"\\s+\", '', sub_label).lower()\n",
    "    rel_label = re.sub(r\"\\s+\", '', rel_label).lower()\n",
    "    obj_label = re.sub(r\"\\s+\", '', obj_label).lower()\n",
    "    # concatanate them \n",
    "    tr_key = f\"{sub_label}{rel_label}{obj_label}\"\n",
    "    return tr_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6322c8-4416-40f1-bc98-cc8efa382ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sent_index = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1477d7d4-c1f3-4123-b2a4-cd28ba55a03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TekGen corpus processing started!\n",
      "\ttriple-to-sent index with 11358950 triples loaded in 2.53 mins!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"TekGen corpus processing started!\")\n",
    "with open('tekgen.csv') as csv_in_file:\n",
    "    sent_reader = csv.reader(csv_in_file)\n",
    "    next(sent_reader)\n",
    "    for row in sent_reader:\n",
    "        tr_key = create_key(row[0], row[1], row[2])\n",
    "        sent = row[4]\n",
    "        sent_index[tr_key] = sent\n",
    "        elapsed_time = (time.time()-start_time)/60\n",
    "    print(f\"\\ttriple-to-sent index with {len(sent_index)} triples loaded in {elapsed_time:.2f} mins!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9cfacfbe-1494-4a2d-a625-5ef0758d6e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ontology_paths = ['5_military_ontology.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c801336-be11-4395-83eb-d889b334289e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ontologies = []\n",
    "for ontology_path in ontology_paths:\n",
    "    with open(ontology_path) as in_file:\n",
    "        ontologies.append(json.load(in_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d277a191-6e9c-4524-b146-726bd5ecffbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology: Military Ontology (ont_5_military)\n",
      "\n",
      "processing \"military rank\" (P410) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Wilhelm von Tegetthoff', 'military rank', 'admiral']\n",
      "    ['Maurice of Nassau', 'military rank', 'admiral']\n",
      "    ['Prince Luigi Amedeo, Duke of the Abruzzi', 'military rank', 'admiral']\n",
      "    ['Roger of Lauria', 'military rank', 'admiral']\n",
      "    ['Kantarō Suzuki', 'military rank', 'admiral']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"military branch\" (P241) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Sean Connery', 'military branch', 'Royal Navy']\n",
      "    ['Mikhail Katukov', 'military branch', 'Red Army']\n",
      "    ['Manuel Blanco Encalada', 'military branch', 'Chilean Navy']\n",
      "    ['Park Chung-hee', 'military branch', 'Republic of Korea Army']\n",
      "    ['Erwin Rommel', 'military branch', 'Imperial German Army']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"military casualty classification \" (P1347) relation:\n",
      "  collected 5664 triples\n",
      "  sample:\n",
      "    ['Eitel-Friedrich Kentrat', 'military casualty classification ', 'prisoner of war']\n",
      "    ['John McCain', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Karl Dönitz', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Klaus Scholtz', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Wolfgang Römer', 'military casualty classification ', 'prisoner of war']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "  collected 569 triples\n",
      "  sample:\n",
      "    ['Type 91 Surface-to-air missile', 'designed by', 'Toshiba']\n",
      "    ['FGM-172 SRAW', 'designed by', 'Lockheed Martin']\n",
      "    ['Mark 44', 'designed by', 'United States Navy']\n",
      "    ['Mark 44', 'designed by', 'General Electric']\n",
      "    ['M39', 'designed by', 'Ford Motor Company']\n",
      "    89 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "  collected 170 triples\n",
      "  sample:\n",
      "    ['BTR-T', 'designed by', 'Omsktransmash']\n",
      "    ['OA vz. 30', 'designed by', 'Tatra']\n",
      "    ['2S23 Nona-SVK', 'designed by', 'TsNIITochMash']\n",
      "    ['Raad-2', 'designed by', 'Defense Industries Organization']\n",
      "    ['automoteur Batignolles-Chatillon 155mm', 'designed by', 'Batignolles-Châtillon']\n",
      "    33 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"commanded by\" (P4791) relation:\n",
      "  collected 170 triples\n",
      "  sample:\n",
      "    ['United States Southern Command', 'commanded by', 'Kurt W. Tidd']\n",
      "    ['Marineabschnittskommando Ostsee', 'commanded by', 'Carl-Heinz Birnbacher']\n",
      "    ['Arctic Command', 'commanded by', 'Martin la Cour-Andersen']\n",
      "    ['Western Military Command', 'commanded by', 'Lennart Ljung']\n",
      "    ['Western Military Command', 'commanded by', 'Nils Personne']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"next higher rank\" (P3730) relation:\n",
      "  collected 312 triples\n",
      "  sample:\n",
      "    ['Ledende spesialist', 'next higher rank', 'Seniorspesialist']\n",
      "    ['captain at sea', 'next higher rank', 'Flottillenadmiral']\n",
      "    ['General of 5th Rank Class', 'next higher rank', 'General of 4th Rank Class']\n",
      "    ['master sergeant', 'next higher rank', 'sergeant major']\n",
      "    ['captain', 'next higher rank', 'major']\n",
      "    4 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"designated as terrorist by\" (P3461) relation:\n",
      "  collected 172 triples\n",
      "  sample:\n",
      "    [\"New People's Army\", 'designated as terrorist by', 'New Zealand']\n",
      "    ['Jaish-e-Mohammed', 'designated as terrorist by', 'India']\n",
      "    ['Jaish-e-Mohammed', 'designated as terrorist by', 'Australia']\n",
      "    ['Jaish-e-Mohammed', 'designated as terrorist by', 'South Africa']\n",
      "    ['Hamas', 'designated as terrorist by', 'Paraguay']\n",
      "    18 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"wing configuration\" (P1654) relation:\n",
      "  collected 408 triples\n",
      "  sample:\n",
      "    ['LFG Roland D.VI', 'wing configuration', 'biplane']\n",
      "    ['IMAM Ro.43', 'wing configuration', 'biplane']\n",
      "    ['Hansa-Brandenburg W.11', 'wing configuration', 'biplane']\n",
      "    ['military biplane with 1 propeller', 'wing configuration', 'biplane']\n",
      "    ['Wright Model K', 'wing configuration', 'biplane']\n",
      "    6 triples with sentences in 0.00 seconds!\n"
     ]
    }
   ],
   "source": [
    "show_sample = True\n",
    "show_query = False\n",
    "\n",
    "for onto in ontologies:\n",
    "    print(f\"Ontology: {onto['title']} ({onto['id']})\")\n",
    "    onto_id = onto['id']\n",
    "    train_all, val_all, test_all = [], [],[]\n",
    "    for rel in onto['relations']:\n",
    "        print(f\"\\nprocessing \\\"{rel['label']}\\\" ({rel['pid']}) relation:\")\n",
    "        start_time = time.time()\n",
    "        triples_with_sentences = get_triples_with_sentences(rel['pid'], rel['label'], rel['domain'], rel['range'], 200)\n",
    "        elapsed_time = (time.time()-start_time)\n",
    "        print(f\"    {len(triples_with_sentences)} triples with sentences in {elapsed_time:.2f} seconds!\")\n",
    "        train, val, test = get_splits(triples_with_sentences)\n",
    "        train_all += train\n",
    "        val_all += val\n",
    "        test_all += test\n",
    "    save_triples(onto_id, train_all, val_all, test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6746ac6-cb3e-4b3c-874b-026c6689c22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
