{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec9d69d7-ab06-49c2-8611-83eb448e9618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0b24be12-5a0a-4bd6-ac13-749ca73e14fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files_recursive(directory):\n",
    "    file_paths = list()\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if file.endswith(\".xml\") and (\"3triples\" in file_path or \"rdf-to-text-generation-test-data-with-refs-en\" in file_path):\n",
    "                file_paths.append(file_path)\n",
    "    # for direc in dirs:\n",
    "    #     file_paths += list_files_recursive(direc)\n",
    "    return file_paths\n",
    "\n",
    "def process_file(file_path, categories, cat_to_rels, cat_to_sents):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    entry_list = list()\n",
    "    entries = root.find('entries')\n",
    "    for entry in entries.findall('entry'):\n",
    "        category = entry.get('category')\n",
    "        categories.add(category)\n",
    "        # eid = entry.get('eid')\n",
    "        # shape = entry.get('shape')\n",
    "        # shape_type = entry.get('shape_type')\n",
    "        # size = entry.get('size')\n",
    "        triples = list()\n",
    "        modifiedtripleset = entry.find('modifiedtripleset')\n",
    "        for mtriple in modifiedtripleset.findall('mtriple'):\n",
    "            splits = mtriple.text.split(\"|\")\n",
    "            rel = splits[1].strip()\n",
    "            rel_set = cat_to_rels.get(category, set())\n",
    "            rel_set.add(rel)\n",
    "            cat_to_rels[category] = rel_set\n",
    "            triples.append([splits[0].strip(), rel, splits[2].strip()])\n",
    "            \n",
    "            \n",
    "        # if len(new_subjects.intersection(seen_subjects)) > 0:\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     seen_subjects.update(new_subjects)\n",
    "            \n",
    "        lex = entry.find('lex')  \n",
    "        if triples and lex.text:\n",
    "            tr_sents = cat_to_sents.get(category, list())\n",
    "            tr_sents.append({\"sent\": lex.text, \"triples\": triples})\n",
    "            cat_to_sents[category] = tr_sents\n",
    "            \n",
    "        entry_list.append(entry)\n",
    "    return entry_list\n",
    "            \n",
    "def save_sentences(cat_id, sents):\n",
    "    #splits = [0.4, 0.3, 0.3]\n",
    "#     sents = np.array(sents)\n",
    "#     indices = np.random.permutation(sents.shape[0])\n",
    "#     train_count = int(sents.shape[0] * splits[0])\n",
    "#     val_count = int(sents.shape[0] * splits[1])\n",
    "#     test_count = sents.shape[0] - train_count - val_count\n",
    "    \n",
    "#     train_sents = sents[indices[:train_count]]\n",
    "#     val_sents= sents[indices[train_count:train_count+val_count]]\n",
    "#     test_sents = sents[indices[train_count+val_count:]]\n",
    "\n",
    "    train_subjects = set()\n",
    "    test_subjects = set()\n",
    "    train_sents = list()\n",
    "    test_sents = list()\n",
    "    for sent in sents:\n",
    "        sent_subjects = {f\"{cat_id}{tr[0]}\" for tr in sent['triples']}\n",
    "        if len(sent_subjects.intersection(test_subjects)) > 0:\n",
    "            test_sents.append(sent)\n",
    "            test_subjects.update(sent_subjects)\n",
    "        elif len(test_sents) < len(train_subjects) * 2 and len(sent_subjects.intersection(train_subjects)) == 0:\n",
    "            test_sents.append(sent)\n",
    "            test_subjects.update(sent_subjects)\n",
    "        else:\n",
    "            train_sents.append(sent)\n",
    "            train_subjects.update(sent_subjects)\n",
    "            \n",
    "    print(f\"{cat_id}: {len(train_sents) + len(test_sents)}\")\n",
    "    print(f\"train: {len(train_sents)}\")\n",
    "    print(f\"test: {len(test_sents)}\\n\")   \n",
    "    \n",
    "    with open(f\"data/train/{cat_id}_train.jsonl\", \"w\") as out_file:\n",
    "        idx = 1\n",
    "        for s in train_sents:\n",
    "            sentence = s['sent'] \n",
    "            triples = s['triples']\n",
    "            data = {\"id\": f\"{cat_id}_train_{idx}\", \"sent\": sentence}\n",
    "            data[\"triples\"] = [{\"sub\": tr[0], \"rel\": tr[1], \"obj\": tr[2]} for tr in triples]\n",
    "            idx += 1\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "                \n",
    "    # with open(f\"data/validation/{cat_id}_validation.jsonl\", \"w\") as out_file:\n",
    "    #     idx = 1\n",
    "    #     for s in val_sents:\n",
    "    #         sentence = s['sent'] \n",
    "    #         triples = s['triples']\n",
    "    #         data = {\"id\": f\"{cat_id}_valid_{idx}\", \"sent\": sentence}\n",
    "    #         data[\"triples\"] = [{\"sub\": tr[0], \"rel\": tr[1], \"obj\": tr[2]} for tr in triples]                \n",
    "    #         idx += 1\n",
    "    #         out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "                \n",
    "    with open(f\"data/test/{cat_id}_test.jsonl\", \"w\") as test_file, open(f\"data/ground_truth/{cat_id}_ground_truth.jsonl\", \"w\") as gt_file:\n",
    "        idx = 1\n",
    "        for s in test_sents:\n",
    "            sentence = s['sent'] \n",
    "            triples = s['triples']\n",
    "            data = {\"id\": f\"{cat_id}_test_{idx}\",  \"sent\": sentence}\n",
    "            test_file.write(f\"{json.dumps(data)}\\n\")\n",
    "            data[\"triples\"] = [{\"sub\": tr[0], \"rel\": tr[1], \"obj\": tr[2]} for tr in triples]\n",
    "            gt_file.write(f\"{json.dumps(data)}\\n\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "83f85b79-1a3b-4201-877a-2de24a3578c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_list = sorted(list_files_recursive(\"webnlg\"), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a1870fc-981f-45f5-8d5e-08a3dbce99c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = set()\n",
    "cat_to_rels = dict()\n",
    "cat_to_sents = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf6a662f-dfe3-4c0e-bb11-01084bdb1ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 4860 entries.\n"
     ]
    }
   ],
   "source": [
    "entry_list = list()\n",
    "seen_triples = set()\n",
    "for file_path in file_list:\n",
    "    entry_list += process_file(file_path, categories, cat_to_rels, cat_to_sents)\n",
    "\n",
    "print(f\"processed {len(entry_list)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2b469ee0-fe59-4800-996b-a27cd4d78c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ont_1_university: 156\n",
      "train: 85\n",
      "test: 71\n",
      "\n",
      "ont_2_musicalwork: 290\n",
      "train: 81\n",
      "test: 209\n",
      "\n",
      "ont_3_airport: 306\n",
      "train: 227\n",
      "test: 79\n",
      "\n",
      "ont_4_building: 275\n",
      "train: 172\n",
      "test: 103\n",
      "\n",
      "ont_5_athlete: 293\n",
      "train: 186\n",
      "test: 107\n",
      "\n",
      "ont_6_politician: 319\n",
      "train: 184\n",
      "test: 135\n",
      "\n",
      "ont_7_company: 153\n",
      "train: 97\n",
      "test: 56\n",
      "\n",
      "ont_8_celestialbody: 194\n",
      "train: 122\n",
      "test: 72\n",
      "\n",
      "ont_9_astronaut: 154\n",
      "train: 86\n",
      "test: 68\n",
      "\n",
      "ont_10_comicscharacter: 102\n",
      "train: 66\n",
      "test: 36\n",
      "\n",
      "ont_11_meanoftransportation: 314\n",
      "train: 222\n",
      "test: 92\n",
      "\n",
      "ont_12_monument: 92\n",
      "train: 73\n",
      "test: 19\n",
      "\n",
      "ont_13_food: 398\n",
      "train: 245\n",
      "test: 153\n",
      "\n",
      "ont_14_writtenwork: 322\n",
      "train: 195\n",
      "test: 127\n",
      "\n",
      "ont_15_sportsteam: 235\n",
      "train: 125\n",
      "test: 110\n",
      "\n",
      "ont_16_city: 348\n",
      "train: 131\n",
      "test: 217\n",
      "\n",
      "ont_17_artist: 386\n",
      "train: 302\n",
      "test: 84\n",
      "\n",
      "ont_18_scientist: 259\n",
      "train: 110\n",
      "test: 149\n",
      "\n",
      "ont_19_film: 264\n",
      "train: 137\n",
      "test: 127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, cat in enumerate(categories):\n",
    "    onto_data = dict()\n",
    "    onto_data[\"title\"] = f\"{cat} Ontology\"\n",
    "    onto_data[\"id\"] = f\"ont_{idx+1}_{cat.lower()}\"\n",
    "    onto_data[\"concepts\"] = [{\"qid\": cat, \"label\": cat}] + [{\"qid\": \"\", \"label\": \"\"} for i in range(len(cat_to_rels[cat]))]\n",
    "    relations = list()\n",
    "    for rel in cat_to_rels[cat]:\n",
    "        relations.append({\"pid\": rel, \"label\": rel, \"domain\": f\"{cat}\", \"range\": rel})\n",
    "    onto_data[\"relations\"] = relations\n",
    "    file_name = onto_data[\"id\"] + \"_ontology.json\"\n",
    "    with open(f\"ontology/{file_name}\", \"w\") as out_file:\n",
    "        json.dump(onto_data, out_file, indent=2)\n",
    "        \n",
    "    save_sentences(onto_data[\"id\"], cat_to_sents[cat])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
