import argparse
import sys
import os
import json
import re
from typing import List, Dict, Set, Tuple
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer


def calculate_precision_recall_f1(gold: Set, pred: Set) -> (float, float, float):
    """
    Method to calculate precision, recall and f1. Precision is calculated as correct_triples/predicted_triples and
    recall as correct_triples/gold_triples, F1 as the harmonic mean of precision and recall.
    :param gold: items in the gold standard
    :param pred: items in the system prediction
    :return:
        p: float - precision
        r: float - recall
        f1: float - F1
    """
    if len(pred) == 0:
        return 0, 0, 0
    p = len(gold.intersection(pred)) / len(pred)
    r = len(gold.intersection(pred)) / len(gold)
    if p + r > 0:
        f1 = 2 * ((p * r) / (p + r))
    else:
        f1 = 0
    return p, r, f1


def get_subject_object_hallucinations(ps, ontology, test_sentence, triples) -> (float, float):
    """
    Calculate subject and object hallucinations metrics. As the context for calculating hallucinations, we consider the
    test sentence and the ontology concepts as relevant tokens.
    :param ps: stemmer for stemming words before checking for hallucinations
    :param ontology: ontology to take into account with the concepts and relations
    :param test_sentence: test sentences for which the triples are generated
    :param triples: a set of triples generated by the system
    :return:
        subj_hallucination: float - subject hallucination metric
        obj_hallucination: float - object hallucination metric
    """

    # if the set of triples are empty, we return 0
    if len(triples) == 0:
        return 0, 0

    # append the test sentence with concepts from the ontology
    test_sentence += " ".join([c["label"] for c in ontology['concepts']])
    # stem each word in the test sentence concatenated with the ontology concepts
    stemmed_sentence = "".join([ps.stem(word) for word in word_tokenize(test_sentence)])
    # normalize the text to remove white spaces and underscores
    normalized_stemmed_sentence = re.sub(r"(_|\s+)", '', stemmed_sentence).lower()

    # count the number of subject and object hallucinations
    num_subj_hallucinations, num_obj_hallucinations = 0, 0
    for triple in triples:
        # clean and normalize subject and object noun phrases the same way as the test sentence
        normalized_stemmed_subject = clean_entity_string(ps, triple[0])
        normalized_stemmed_object = clean_entity_string(ps, triple[2])

        # check if the subject/object is found in the context text. If not found, mark it as a hallucination
        if normalized_stemmed_sentence.find(normalized_stemmed_subject) == -1:
            num_subj_hallucinations += 1
        if normalized_stemmed_sentence.find(normalized_stemmed_object) == -1:
            num_obj_hallucinations += 1

    # divide the number of hallucinations by the number of triples to calculate the hallucination metrics
    subj_hallucination = num_subj_hallucinations / len(triples)
    obj_hallucination = num_obj_hallucinations / len(triples)
    return subj_hallucination, obj_hallucination


def get_ontology_conformance(ontology: Dict, triples: List) -> (float, float):
    """
    Calculate the ontology conformance and relation hallucination metrics.
    :param ontology: ontology to take into account with the concepts and relations
    :param triples: a set of triples generated by the system
    :return:
        ont_conformance: float - ontology conformance metric
        rel_hallucination: float - relation hallucination metric
    """
    if len(triples) == 0:
        return 1, 0
    ont_rels = [rel['label'].replace(" ", "_") for rel in ontology['relations']]
    num_rels_conformant = len([tr for tr in triples if tr[1] in ont_rels])
    ont_conformance = num_rels_conformant / len(triples)
    rel_hallucination = 1 - ont_conformance
    return ont_conformance, rel_hallucination


def normalize_triple(sub_label: str, rel_label: str, obj_label: str) -> str:
    """
    Normalize triples for comparison in precision, recall calculations
    :param sub_label: subject string
    :param rel_label: relation string
    :param obj_label: object string
    :return: a normalized triple as a single concatenated string
    """
    # remove spaces and make lower case
    sub_label = re.sub(r"(_|\s+)", '', sub_label).lower()
    rel_label = re.sub(r"(_|\s+)", '', rel_label).lower()
    obj_label = re.sub(r"(_|\s+)", '', obj_label).lower()
    # concatenate them to a single string
    tr_key = f"{sub_label}{rel_label}{obj_label}"
    return tr_key


def clean_entity_string(ps, entity: str) -> str:
    """
    Utility method to clean subject and object strings of triples
    :param ps: stemmer for stemming words before checking for hallucinations
    :param entity: subject or object string
    :return: the cleaned and normalized string
    """
    # stem every word for better matches
    stemmed_entity = "".join([ps.stem(word) for word in word_tokenize(entity)])
    # normalizing the string by removing white spaces, underscores and then converting to lower case
    normalized_stemmed_entity = re.sub(r"(_|\s+)", '', stemmed_entity).lower()
    # special handling for string with years to remove January 01
    return normalized_stemmed_entity.replace("01januari", "")


def read_jsonl(jsonl_path: str, is_json: bool = True) -> List:
    """
    Utility method to read lines from .jsonl file to a data list
    :param jsonl_path: path to the .jsonl file
    :param is_json: a flag to indicate if each line is a json dictionary
    :return: a list of strings or json objects containing data in each line
    """
    data = []
    with open(jsonl_path) as in_file:
        for line in in_file:
            if is_json:
                data.append(json.loads(line))
            else:
                data.append(line.strip())
    return data


def load_config(eval_config_path: str) -> Dict:
    """
    Load the evaluation configuration file
    :param eval_config_path: path to the evaluation configuration file
    :return: a new config object where paths to the files are resolved based on the path patterns
    """
    raw_config = read_json(eval_config_path)
    onto_list = raw_config['onto_list']
    path_patterns = raw_config["path_patterns"]
    new_config = dict()
    expanded_onto_list = list()
    for onto in onto_list:
        onto_data = dict()
        onto_data["id"] = onto
        for key in path_patterns:
            onto_data[key] = path_patterns[key].replace("$$onto$$", onto)
        expanded_onto_list.append(onto_data)
    new_config["onto_list"] = expanded_onto_list
    new_config["avg_out_file"] = raw_config["avg_out_file"]
    return new_config


def save_jsonl(data: List, jsonl_path: str) -> None:
    """
    Utility method to serialize a list of json objects to a .jsonl file
    :param data: list of data items
    :param jsonl_path: path to the output .jsonl file
    :return: None
    """
    with open(jsonl_path, "w") as out_file:
        for item in data:
            out_file.write(f"{json.dumps(item)}\n")


def append_jsonl(data: Dict, jsonl_path: str) -> None:
    """
    Utility method to append a new line to a .jsonl file
    :param data: data to be serialized into the file
    :param jsonl_path: path of the file to be appended
    :return: None
    """
    with open(jsonl_path, "a+") as out_file:
        out_file.write(f"{json.dumps(data)}\n")


def read_json(json_path: str) -> Dict:
    """
    Utility method for reading JSON files
    :param json_path: path to the json file
    :return: json file content as a dictionary
    """
    with open(json_path) as in_file:
        return json.load(in_file)


def convert_to_dict(data: List[Dict], id_name: str = "id") -> Dict:
    """
    Utility method to convert a list to a dictionary
    :param data: a list of dictionary objects
    :param id_name: the attribute to be used as the key for the dictionary
    :return: a dictionary with the same content as the list
    """
    return {item[id_name]: item for item in data}


def main():
    parser = argparse.ArgumentParser()
    # please have a look at src/evaluation/config for examples of evaluation configs.
    parser.add_argument('--eval_config_path', type=str, required=True)
    args = parser.parse_args()

    # stemmer for stemming words before checking for hallucinations
    ps = PorterStemmer()

    # load the files needed for evaluation from a user provided config file, it contains the system generated
    # output, the ground truth files, path to ontology file, and the path to store the evaluation output.
    eval_config_path = args.eval_config_path
    if not os.path.exists(eval_config_path):
        print(f"Evaluation config file is not found in path: {eval_config_path}")
    eval_inputs = load_config(eval_config_path)

    global_p, global_r, global_f1, global_onto_conf, global_rel_halluc, global_sub_halluc, global_obj_halluc = 0, 0, 0, 0, 0, 0, 0
    # evaluate the output of each of the ontologies
    for onto in eval_inputs['onto_list']:
        t_p, t_r, t_f1, t_onto_conf, t_rel_halluc, t_sub_halluc, t_obj_halluc = 0, 0, 0, 0, 0, 0, 0
        sel_t_p, sel_t_r, sel_t_f1, sel_t_onto_conf, sel_t_rel_halluc, sel_t_sub_halluc, sel_t_obj_halluc = \
            0, 0, 0, 0, 0, 0, 0
        eval_metrics_list = list()
        onto_id = onto['id']
        system_output = convert_to_dict(read_jsonl(onto['sys']))
        ground_truth = convert_to_dict(read_jsonl(onto['gt']))
        ontology = read_json(onto['onto'])
        if 'selected_ids' in onto:
            selected_ids = read_jsonl(onto['selected_ids'], is_json=False)
        else:
            selected_ids = []

        # iterate through each element in the ground truth and evaluate the system output
        for sent_id in list(ground_truth.keys()):
            # collect the ground truth triples
            gt_triples = [[tr['sub'], tr['rel'], tr['obj']] for tr in ground_truth[sent_id]['triples']]
            sentence = ground_truth[sent_id]["sent"]

            # check if system output as an entry for this sentence
            if sent_id in system_output:
                system_triples = system_output[sent_id]['triples']

                # collect the set of relations in ground truth triples, spaces are converted to "_" to make them
                # comparable with system triples
                gt_relations = {tr[1].replace(" ", "_") for tr in gt_triples}

                # filter out any triples in system output that does not match with ground truth relations
                filtered_system_triples = [tr for tr in system_triples if tr[1] in gt_relations]

                # create a normalized string from subject, relation, object of each triple for comparison
                normalized_system_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in filtered_system_triples}
                normalized_gt_triples = {normalize_triple(tr[0], tr[1], tr[2]) for tr in gt_triples}

                # compare the system output with ground truth triples and calculate precision, recall, f1
                precision, recall, f1 = calculate_precision_recall_f1(normalized_gt_triples, normalized_system_triples)

                ont_conformance, rel_hallucination = get_ontology_conformance(ontology, system_triples)
                subj_hallucination, obj_hallucination = get_subject_object_hallucinations(ps, ontology, sentence,
                                                                                          system_triples)
                if  f1 < 1  and len(filtered_system_triples) > 0 and subj_hallucination == 0 and obj_hallucination == 0:
                    print(f"sent: {sentence}\nf1: {f1}\nsys:{filtered_system_triples}\nground:{gt_triples}\n\n")

                eval_metrics = {"id": sent_id, "precision": f"{precision:.2f}" , "recall": f"{recall:.2f}", "f1": f"{f1:.2f}",
                                "onto_conf": f"{ont_conformance:.2f}", "rel_halluc": f"{rel_hallucination:.2f}",
                                "sub_halluc": f"{subj_hallucination:.2f}", "obj_halluc": f"{obj_hallucination:.2f}",
                                "llm_triples": system_triples, "filtered_llm_triples": filtered_system_triples,
                                "gt_triples": gt_triples, "sent": sentence}
                eval_metrics_list.append(eval_metrics)

                # aggregate precision, recall, f1 for later averaging
                t_p += precision
                t_r += recall
                t_f1 += f1
                t_onto_conf += ont_conformance
                t_rel_halluc += rel_hallucination
                t_sub_halluc += subj_hallucination
                t_obj_halluc += obj_hallucination

                # aggregate precision, recall, f1 for later averaging for selected ids
                if sent_id in selected_ids:
                    sel_t_p += precision
                    sel_t_r += recall
                    sel_t_f1 += f1
                    sel_t_onto_conf += ont_conformance
                    sel_t_rel_halluc += rel_hallucination
                    sel_t_sub_halluc += subj_hallucination
                    sel_t_obj_halluc += obj_hallucination

        save_jsonl(eval_metrics_list, onto['output'])
        total_test_cases = len(ground_truth)
        total_selected_test_cases = len(selected_ids)
        # average metrics calculate the average of evaluate metrics for all test cases in a given ontology
        average_metrics = {"onto": onto_id, "type": "all_test_cases",
                           "avg_precision": f"{t_p/total_test_cases:.2f}",
                           "avg_recall": f"{t_r/total_test_cases:.2f}",
                           "avg_f1": f"{t_f1/total_test_cases:.2f}",
                           "avg_onto_conf": f"{t_onto_conf/total_test_cases:.2f}",
                           "avg_sub_halluc": f"{t_sub_halluc/total_test_cases:.2f}",
                           "avg_rel_halluc": f"{t_rel_halluc / total_test_cases:.2f}",
                           "avg_obj_halluc": f"{t_obj_halluc/total_test_cases:.2f}"
                           }
        append_jsonl(average_metrics, eval_inputs['avg_out_file'])
        global_p += (t_p/total_test_cases)
        global_r += (t_r/total_test_cases)
        global_f1 += (t_f1/total_test_cases)
        global_onto_conf += (t_onto_conf/total_test_cases)
        global_sub_halluc += (t_sub_halluc/total_test_cases)
        global_rel_halluc += (t_rel_halluc / total_test_cases)
        global_obj_halluc += (t_obj_halluc/total_test_cases)
        # in some cases, we have a subset of selected test cases for which we report the average numbers separately
        if total_selected_test_cases > 0:
            selected_average_metrics = {"onto": onto_id, "type": "selected_test_cases",
                                        "avg_precision": f"{sel_t_p /total_selected_test_cases:.2f}",
                                        "avg_recall": f"{sel_t_r/total_selected_test_cases:.2f}",
                                        "avg_f1": f"{sel_t_f1/total_selected_test_cases:.2f}",
                                        "avg_onto_conf": f"{sel_t_onto_conf / total_selected_test_cases:.2f}",
                                        "avg_sub_halluc": f"{sel_t_sub_halluc / total_selected_test_cases:.2f}",
                                        "avg_rel_halluc": f"{sel_t_rel_halluc / total_selected_test_cases:.2f}",
                                        "avg_obj_halluc": f"{sel_t_obj_halluc / total_selected_test_cases:.2f}"}
            append_jsonl(selected_average_metrics, eval_inputs['avg_out_file'])

    # global metrics calculate the average metrics for all ontologies that are part of the evaluation
    num_ontologies = len(eval_inputs['onto_list'])
    global_metrics = {"id": "global", "type": "global",
                      "avg_precision": f"{global_p / num_ontologies:.2f}",
                      "avg_recall": f"{global_r / num_ontologies:.2f}",
                      "avg_f1": f"{global_f1 / num_ontologies:.2f}",
                      "avg_onto_conf": f"{global_onto_conf / num_ontologies:.2f}",
                      "avg_sub_halluc": f"{global_sub_halluc / num_ontologies:.2f}",
                      "avg_rel_halluc": f"{global_rel_halluc / num_ontologies:.2f}",
                      "avg_obj_halluc": f"{global_obj_halluc / num_ontologies:.2f}",
                      "onto_list": eval_inputs['onto_list']}
    append_jsonl(global_metrics, eval_inputs['avg_out_file'])


if __name__ == "__main__":
    sys.exit(main())